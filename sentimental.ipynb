{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import  CuDNNLSTM,Bidirectional, GlobalMaxPool1D,Input,Conv1D,MaxPooling1D,Embedding,Reshape,TimeDistributed,Dense, Activation, LSTM, SimpleRNN, Dropout  \n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD,Adam,RMSprop,Adadelta\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    504291\n",
       "0    431145\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.200d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "label = []\n",
    "for i,j in zip(review['comm'],review['label']):\n",
    "    texts.append(str(i).lower())\n",
    "    label.append(int(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(tokenizer, open( \"tokenizer.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76753 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "num_words = min(MAX_NB_WORDS, len(word_index)) +1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_4_input (InputLayer)  (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1000)         0           embedding_4_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1000)         0           embedding_4_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1000)         0           embedding_4_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1000)         0           embedding_4_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 1)            7187401     lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Concatenate)           (None, 1)            0           sequential_4[1][0]               \n",
      "                                                                 sequential_4[2][0]               \n",
      "                                                                 sequential_4[3][0]               \n",
      "                                                                 sequential_4[4][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,187,401\n",
      "Trainable params: 3,187,201\n",
      "Non-trainable params: 4,000,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding(num_words,\n",
    "                     EMBEDDING_DIM,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False))\n",
    "model.add(Bidirectional(CuDNNLSTM(512, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# model.add(LSTM(units=512,dropout=0.2,return_sequences=False))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model = multi_gpu_model(model, 4)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max',  period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 926081 samples, validate on 9355 samples\n",
      "Epoch 1/15\n",
      "926081/926081 [==============================] - 490s 529us/step - loss: 0.2788 - acc: 0.8879 - val_loss: 0.2440 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00001: saving model to weights-01-0.91.hdf5\n",
      "Epoch 2/15\n",
      "926081/926081 [==============================] - 489s 528us/step - loss: 0.2390 - acc: 0.9090 - val_loss: 0.2264 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00002: saving model to weights-02-0.92.hdf5\n",
      "Epoch 3/15\n",
      "926081/926081 [==============================] - 490s 529us/step - loss: 0.2196 - acc: 0.9180 - val_loss: 0.2265 - val_acc: 0.9152\n",
      "\n",
      "Epoch 00003: saving model to weights-03-0.92.hdf5\n",
      "Epoch 4/15\n",
      "926081/926081 [==============================] - 490s 530us/step - loss: 0.2094 - acc: 0.9225 - val_loss: 0.2206 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00004: saving model to weights-04-0.92.hdf5\n",
      "Epoch 5/15\n",
      "926081/926081 [==============================] - 491s 530us/step - loss: 0.1995 - acc: 0.9266 - val_loss: 0.2182 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00005: saving model to weights-05-0.92.hdf5\n",
      "Epoch 6/15\n",
      "926081/926081 [==============================] - 490s 529us/step - loss: 0.1886 - acc: 0.9308 - val_loss: 0.2211 - val_acc: 0.9178\n",
      "\n",
      "Epoch 00006: saving model to weights-06-0.92.hdf5\n",
      "Epoch 7/15\n",
      "926081/926081 [==============================] - 491s 530us/step - loss: 0.1775 - acc: 0.9350 - val_loss: 0.2234 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00007: saving model to weights-07-0.92.hdf5\n",
      "Epoch 8/15\n",
      "926081/926081 [==============================] - 492s 531us/step - loss: 0.1643 - acc: 0.9395 - val_loss: 0.2437 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00008: saving model to weights-08-0.91.hdf5\n",
      "Epoch 9/15\n",
      "926081/926081 [==============================] - 491s 530us/step - loss: 0.1505 - acc: 0.9442 - val_loss: 0.2442 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00009: saving model to weights-09-0.92.hdf5\n",
      "Epoch 10/15\n",
      "926081/926081 [==============================] - 492s 531us/step - loss: 0.1373 - acc: 0.9487 - val_loss: 0.2474 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00010: saving model to weights-10-0.92.hdf5\n",
      "Epoch 11/15\n",
      "926081/926081 [==============================] - 492s 531us/step - loss: 0.1238 - acc: 0.9533 - val_loss: 0.2880 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00011: saving model to weights-11-0.91.hdf5\n",
      "Epoch 12/15\n",
      "926081/926081 [==============================] - 493s 532us/step - loss: 0.1106 - acc: 0.9577 - val_loss: 0.3552 - val_acc: 0.8900\n",
      "\n",
      "Epoch 00012: saving model to weights-12-0.89.hdf5\n",
      "Epoch 13/15\n",
      "926081/926081 [==============================] - 492s 532us/step - loss: 0.1012 - acc: 0.9609 - val_loss: 0.3348 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00013: saving model to weights-13-0.91.hdf5\n",
      "Epoch 14/15\n",
      "926081/926081 [==============================] - 492s 532us/step - loss: 0.0930 - acc: 0.9638 - val_loss: 0.3898 - val_acc: 0.8988\n",
      "\n",
      "Epoch 00014: saving model to weights-14-0.90.hdf5\n",
      "Epoch 15/15\n",
      "926081/926081 [==============================] - 492s 531us/step - loss: 0.0858 - acc: 0.9667 - val_loss: 0.3713 - val_acc: 0.9036\n",
      "\n",
      "Epoch 00015: saving model to weights-15-0.90.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0bca9c23c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, label, batch_size=2048, epochs=15, callbacks=[TensorBoard(log_dir='log_dir'),checkpoint],validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(data, label, batch_size=128, epochs=15, callbacks=[TensorBoard(log_dir='log_dir'),checkpoint],validation_split=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 748348 samples, validate on 187088 samples\n",
      "Epoch 1/5\n",
      "748348/748348 [==============================] - 2998s 4ms/step - loss: 0.2626 - acc: 0.8981 - val_loss: 0.2380 - val_acc: 0.9098\n",
      "Epoch 2/5\n",
      "748348/748348 [==============================] - 3026s 4ms/step - loss: 0.2299 - acc: 0.9139 - val_loss: 0.2302 - val_acc: 0.9131\n",
      "Epoch 3/5\n",
      "748348/748348 [==============================] - 3028s 4ms/step - loss: 0.2133 - acc: 0.9215 - val_loss: 0.2273 - val_acc: 0.9139\n",
      "Epoch 4/5\n",
      "748348/748348 [==============================] - 3025s 4ms/step - loss: 0.1978 - acc: 0.9281 - val_loss: 0.2210 - val_acc: 0.9175\n",
      "Epoch 5/5\n",
      "748348/748348 [==============================] - 3022s 4ms/step - loss: 0.1813 - acc: 0.9347 - val_loss: 0.2297 - val_acc: 0.9153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8eed43ec18>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(data, label, batch_size=128, epochs=5,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentimental4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model =  load_model('sentimental.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def remove_emoji(text):\n",
    "#     emoji_pattern = re.compile(\n",
    "#         \"[\"\n",
    "#         \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            \"]+\"\n",
    "#        , flags=re.UNICODE)\n",
    "#     return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'data'\n",
    "# file = listdir(path)\n",
    "# raw_data= {'label':[],'comm':[]}\n",
    "# for i in tqdm(file):\n",
    "#         fullpath = join(path,i)\n",
    "#         fullpath = fullpath.replace('\\\\', '/') #windows\"\n",
    "#         review = pd.read_csv(fullpath)\n",
    "#         for label,comm in zip(review['label'],review['comm'] ) :\n",
    "#             label = int(label)\n",
    "#             comm = remove_emoji(str(comm))\n",
    "#             raw_data['label'].append(label)\n",
    "#             raw_data['comm'].append(comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(raw_data, columns = ['label','comm'])\n",
    "# df.to_csv('all_data.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
